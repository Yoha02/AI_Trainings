{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Lab 3: Build a RAG PDF Chatbot\n",
        "\n",
        "Welcome to Lab 3! This is the culmination of everything we've learned in this series. Now you'll build a complete **Retrieval Augmented Generation** application that can:\n",
        "\n",
        "-  Upload and process PDF documents\n",
        "-  Convert document chunks into embeddings\n",
        "-  Store vectors in Qdrant database\n",
        "-  Chat with your documents using Gemini\n",
        "-  Present it all in a beautiful Streamlit UI\n",
        "\n",
        "**What you'll learn:**\n",
        "- How RAG works end-to-end\n",
        "- PDF text extraction and chunking\n",
        "- Combining vector search with LLM generation\n",
        "- Building interactive AI applications with Streamlit\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed Lab 1 (LLM basics) and Lab 2 (Vector databases)\n",
        "- Google Cloud account with Vertex AI enabled\n",
        "- Qdrant Cloud account(Free tear)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Understanding RAG (Retrieval Augmented Generation)\n",
        "\n",
        "### What is RAG?\n",
        "\n",
        "**RAG** is a technique that enhances LLM responses by providing relevant context from your own documents. Instead of relying solely on the LLM's training data, RAG retrieves specific information to answer questions accurately.\n",
        "\n",
        "### How RAG Works (The Pipeline):\n",
        "\n",
        "```\n",
        "\n",
        "â”‚  1. INGESTION (One-time setup)                                  â”‚\n",
        "â”‚     PDF â†’ Extract Text â†’ Split into Chunks â†’ Create Embeddings  â”‚\n",
        "â”‚           â†’ Store in Vector Database                            â”‚\n",
        " -----------------------------------------------------------------\n",
        "                              â†“\n",
        "\n",
        "â”‚  2. RETRIEVAL (When user asks a question)                       â”‚\n",
        "â”‚     User Question â†’ Create Embedding â†’ Search Vector DB         â”‚\n",
        "â”‚           â†’ Get Top-K Similar Chunks                            â”‚\n",
        " -----------------------------------------------------------------\n",
        "                              â†“\n",
        "\n",
        "â”‚  3. GENERATION (Create the answer)                              â”‚\n",
        "â”‚     Retrieved Chunks + User Question â†’ Send to LLM â†’ Answer     â”‚\n",
        " -----------------------------------------------------------------\n",
        "```\n",
        "\n",
        "### Why RAG?\n",
        "\n",
        "|        Without RAG              |               With RAG                        |\n",
        "|---------------------------------|-----------------------------------------------|\n",
        "| LLM only knows training data    | LLM has access to **your specific documents** |\n",
        "| May hallucinate facts           | **Grounded** in actual document content       |\n",
        "| Can't answer about private data | Can answer about **any uploaded document**    |\n",
        "| Generic responses               | **Specific, accurate** responses              |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Step 1: Install Required Libraries\n",
        "\n",
        "We need the following libraries for our RAG application:\n",
        "- `qdrant-client`: Vector database client\n",
        "- `google-generativeai`: Google's Gemini API\n",
        "- `google-cloud-aiplatform`: Vertex AI for embeddings\n",
        "- `PyPDF2`: PDF text extraction\n",
        "- `streamlit`: Web UI framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: qdrant-client in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.16.2)\n",
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.6-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.132.0)\n",
            "Collecting PyPDF2\n",
            "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting streamlit\n",
            "  Using cached streamlit-1.52.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (2.4.0)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (6.33.2)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (2.12.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (2.6.2)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (2.28.1)\n",
            "Collecting google-api-python-client (from google-generativeai)\n",
            "  Using cached google_api_python_client-2.187.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (2.45.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.0)\n",
            "Collecting protobuf>=3.20.0 (from qdrant-client)\n",
            "  Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: packaging>=14.3 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (3.7.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (3.39.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (1.56.0)\n",
            "Requirement already satisfied: docstring_parser<1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Collecting altair!=5.4.0,!=5.4.1,<7,>=4.0 (from streamlit)\n",
            "  Using cached altair-6.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
            "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (6.2.4)\n",
            "Collecting click<9,>=7.0 (from streamlit)\n",
            "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (2.3.3)\n",
            "Collecting pillow<13,>=7.1.0 (from streamlit)\n",
            "  Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (22.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (2.32.5)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (9.1.2)\n",
            "Collecting toml<2,>=0.10.1 (from streamlit)\n",
            "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (6.5.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Collecting jsonschema>=3.0 (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting narwhals>=1.27.1 (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Using cached narwhals-2.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.8.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.12.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.9.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: certifi in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client) (311)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
            "  Using cached httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
            "  Using cached google_auth_httplib2-0.3.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
            "  Using cached uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Using cached grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Collecting pyparsing<4,>=3.0.4 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
            "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Using cached rpds_py-0.30.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\asggm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Using cached google_generativeai-0.8.6-py3-none-any.whl (155 kB)\n",
            "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Using cached streamlit-1.52.2-py3-none-any.whl (9.0 MB)\n",
            "Using cached altair-6.0.0-py3-none-any.whl (795 kB)\n",
            "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
            "   ---------------------------------------- 0.0/208.6 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/208.6 kB ? eta -:--:--\n",
            "   ----------------- ---------------------- 92.2/208.6 kB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 208.6/208.6 kB 2.5 MB/s eta 0:00:00\n",
            "Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
            "Using cached protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
            "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
            "Using cached google_api_python_client-2.187.0-py3-none-any.whl (14.6 MB)\n",
            "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Using cached google_auth_httplib2-0.3.0-py3-none-any.whl (9.5 kB)\n",
            "Using cached grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
            "Using cached httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
            "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
            "Using cached narwhals-2.14.0-py3-none-any.whl (430 kB)\n",
            "Using cached uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
            "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
            "Using cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
            "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
            "Using cached rpds_py-0.30.0-cp311-cp311-win_amd64.whl (236 kB)\n",
            "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, uritemplate, toml, smmap, rpds-py, PyPDF2, pyparsing, protobuf, pillow, narwhals, click, blinker, referencing, pydeck, httplib2, gitdb, jsonschema-specifications, grpcio-status, google-auth-httplib2, gitpython, jsonschema, google-api-python-client, google-ai-generativelanguage, altair, streamlit, google-generativeai\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.33.2\n",
            "    Uninstalling protobuf-6.33.2:\n",
            "      Successfully uninstalled protobuf-6.33.2\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.76.0\n",
            "    Uninstalling grpcio-status-1.76.0:\n",
            "      Successfully uninstalled grpcio-status-1.76.0\n",
            "Successfully installed PyPDF2-3.0.1 altair-6.0.0 blinker-1.9.0 click-8.3.1 gitdb-4.0.12 gitpython-3.1.46 google-ai-generativelanguage-0.6.15 google-api-python-client-2.187.0 google-auth-httplib2-0.3.0 google-generativeai-0.8.6 grpcio-status-1.71.2 httplib2-0.31.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 narwhals-2.14.0 pillow-12.0.0 protobuf-5.29.5 pydeck-0.9.1 pyparsing-3.3.1 referencing-0.37.0 rpds-py-0.30.0 smmap-5.0.2 streamlit-1.52.2 toml-0.10.2 uritemplate-4.2.0 watchdog-6.0.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            " All packages installed!\n",
            "\n",
            " If this is your first time, restart the kernel before continuing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\asggm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\~upb'.\n",
            "  You can safely remove it manually.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\asggm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#  Install all required packages (run once)\n",
        "%pip install qdrant-client google-generativeai google-cloud-aiplatform PyPDF2 streamlit\n",
        "\n",
        "print(\" All packages installed!\")\n",
        "print(\"\\n If this is your first time, restart the kernel before continuing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Step 2: Import Libraries\n",
        "\n",
        "Let's import all the libraries we need. Each serves a specific purpose in our RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "# Google AI imports\n",
        "import google.generativeai as genai  # Gemini for text generation\n",
        "import vertexai                       # Vertex AI platform\n",
        "from vertexai.language_models import TextEmbeddingModel  # For creating embeddings\n",
        "\n",
        "# Qdrant imports\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "\n",
        "# PDF processing\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# UI (will be used in the Streamlit app)\n",
        "# import streamlit as st  # Uncomment when running as Streamlit app\n",
        "\n",
        "print(\" All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Step 3: Configure Services\n",
        "\n",
        "We need to set up connections to:\n",
        "1. **Google AI (Gemini)** - For text generation\n",
        "2. **Vertex AI** - For embeddings\n",
        "3. **Qdrant** - For vector storage\n",
        "\n",
        "### Configure Google Generative AI (Gemini)\n",
        "\n",
        "Google's Gemini models are accessed via Application Default Credentials (ADC) when running in Google Cloud. For local development, you may need to set up authentication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Configured with Application Default Credentials (ADC)\n",
            " Vertex AI initialized for project: test_project\n"
          ]
        }
      ],
      "source": [
        "# \n",
        "#   GOOGLE CLOUD CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "# Option 1: If running in Google Cloud (Vertex AI Workbench), ADC works automatically\n",
        "# Option 2: For local development, set your API key below\n",
        "\n",
        "# For local development, get an API key from: https://makersuite.google.com/app/apikey\n",
        "GOOGLE_API_KEY = \"\"  # Leave empty if using ADC in Google Cloud\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\" Configured with API key\")\n",
        "else:\n",
        "    genai.configure()  # Uses Application Default Credentials\n",
        "    print(\" Configured with Application Default Credentials (ADC)\")\n",
        "\n",
        "# Initialize Vertex AI for embeddings\n",
        "# Replace with your Google Cloud Project ID if not using environment variable\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"test_project\")\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
        "print(f\" Vertex AI initialized for project: {PROJECT_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Qdrant Connection\n",
        "\n",
        "Enter your Qdrant Cloud credentials (you can use same credentails as Lab 2).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Qdrant credentials configured\n",
            " URL: https://d78b1147-cde0-4b94-aa1e-9b6b2278050c.us-ea...\n"
          ]
        }
      ],
      "source": [
        "#   QDRANT CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "# Your Qdrant API Key (from https://cloud.qdrant.io)\n",
        "QDRANT_API_KEY = \"\"  # Paste your API key here\n",
        "\n",
        "# Your Qdrant Cluster URL (make sure port is 6333!)\n",
        "QDRANT_URL = \"\"  # Example: \"https://xxx.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
        "\n",
        "# ============================================\n",
        "\n",
        "# Validate Qdrant credentials\n",
        "if not QDRANT_API_KEY or not QDRANT_URL:\n",
        "    print(\" ERROR: Missing Qdrant credentials!\")\n",
        "    print(\"   Please fill in QDRANT_API_KEY and QDRANT_URL above.\")\n",
        "else:\n",
        "    print(\" Qdrant credentials configured\")\n",
        "    print(f\" URL: {QDRANT_URL[:50]}...\" if len(QDRANT_URL) > 50 else f\"ðŸ”— URL: {QDRANT_URL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure LLM Settings\n",
        "\n",
        "These settings control how the Gemini model generates responses.\n",
        "\n",
        "**Parameters explained:**\n",
        "- **MODEL_NAME**: Which Gemini model to use\n",
        "- **TEMPERATURE**: Controls randomness (0 = deterministic, 1 = creative)\n",
        "- **TOP_P**: Controls diversity of responses\n",
        "- **MAX_OUTPUT**: Maximum tokens in the response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model: gemini-2.0-flash-001\n",
            "  Temperature: 0.7\n",
            "  Top-P: 0.9\n",
            " Max Output Tokens: 8192\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  LLM CONFIGURATION\n",
        "# =====================\n",
        "\n",
        "# Model options: \"gemini-2.0-flash-001\", \"gemini-1.5-flash\", \"gemini-1.5-pro\"\n",
        "MODEL_NAME = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Temperature: 0.0 (factual) to 1.0 (creative)\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "# Top-P: Controls diversity (0.1 to 1.0)\n",
        "TOP_P = 0.9\n",
        "\n",
        "# Maximum output tokens\n",
        "MAX_OUTPUT = 8192\n",
        "\n",
        "# Default collection name for PDFs\n",
        "DEFAULT_COLLECTION = \"pdfs_collection\"\n",
        "\n",
        "# ============================================\n",
        "\n",
        "print(f\" Model: {MODEL_NAME}\")\n",
        "print(f\"  Temperature: {TEMPERATURE}\")\n",
        "print(f\"  Top-P: {TOP_P}\")\n",
        "print(f\" Max Output Tokens: {MAX_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Step 4: Define Core RAG Functions\n",
        "\n",
        "Now we'll create the core functions that power our RAG application. These handle:\n",
        "1. **Qdrant connection** - Initialize database client\n",
        "2. **Collection management** - Create/recreate vector collections\n",
        "3. **PDF ingestion** - Extract text, chunk it, create embeddings, store in Qdrant\n",
        "4. **Response generation** - Retrieve context and generate answers\n",
        "\n",
        "### 4.1 Qdrant Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Qdrant helper functions defined\n"
          ]
        }
      ],
      "source": [
        "def init_qdrant(qdrant_url: str, qdrant_api_key: str) -> QdrantClient:\n",
        "    \"\"\"\n",
        "    Initialize and return a Qdrant client.\n",
        "    \n",
        "    Args:\n",
        "        qdrant_url: Your Qdrant cluster URL\n",
        "        qdrant_api_key: Your Qdrant API key\n",
        "    \n",
        "    Returns:\n",
        "        QdrantClient instance\n",
        "    \"\"\"\n",
        "    return QdrantClient(url=qdrant_url, api_key=qdrant_api_key, timeout=30)\n",
        "\n",
        "\n",
        "def create_qdrant_collection(collection_name: str, qdrant_url: str, qdrant_api_key: str, \n",
        "                             vector_size: int = 768, distance: str = \"Cosine\"):\n",
        "    \"\"\"\n",
        "    Create (or recreate) a Qdrant collection for storing document embeddings.\n",
        "    \n",
        "    Args:\n",
        "        collection_name: Name for the collection\n",
        "        qdrant_url: Qdrant cluster URL\n",
        "        qdrant_api_key: Qdrant API key\n",
        "        vector_size: Dimension of embedding vectors (768 for gemini-embedding-001)\n",
        "        distance: Distance metric for similarity (Cosine, Euclidean, or Dot)\n",
        "    \"\"\"\n",
        "    client = init_qdrant(qdrant_url, qdrant_api_key)\n",
        "    \n",
        "    # Try to recreate (deletes if exists, then creates)\n",
        "    try:\n",
        "        client.recreate_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(size=vector_size, distance=distance)\n",
        "        )\n",
        "    except AttributeError:\n",
        "        # Fallback for older Qdrant versions\n",
        "        try:\n",
        "            client.delete_collection(collection_name=collection_name)\n",
        "        except:\n",
        "            pass  # Collection didn't exist\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(size=vector_size, distance=distance)\n",
        "        )\n",
        "    \n",
        "    print(f\" Collection '{collection_name}' created successfully!\")\n",
        "\n",
        "print(\" Qdrant helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 PDF Ingestion Function\n",
        "\n",
        "This is the **Ingestion** phase of RAG. The function:\n",
        "1. Reads PDF files and extracts text\n",
        "2. Splits text into smaller chunks (for better retrieval)\n",
        "3. Creates embeddings for each chunk using Google's embedding model\n",
        "4. Stores chunks and embeddings in Qdrant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PDF ingestion function defined\n"
          ]
        }
      ],
      "source": [
        "def ingest_pdfs_to_qdrant(pdf_files, collection_name: str, qdrant_url: str, \n",
        "                          qdrant_api_key: str, chunk_size: int = 500):\n",
        "    \"\"\"\n",
        "    Ingest PDF files into Qdrant vector database.\n",
        "    \n",
        "    This function:\n",
        "    1. Extracts text from each PDF\n",
        "    2. Splits text into chunks of specified size\n",
        "    3. Creates embeddings using Google's gemini-embedding-001 model\n",
        "    4. Stores chunks with embeddings in Qdrant\n",
        "    \n",
        "    Args:\n",
        "        pdf_files: List of PDF file paths or file objects\n",
        "        collection_name: Name of the Qdrant collection\n",
        "        qdrant_url: Qdrant cluster URL\n",
        "        qdrant_api_key: Qdrant API key\n",
        "        chunk_size: Number of characters per chunk (default: 500)\n",
        "    \"\"\"\n",
        "    client = init_qdrant(qdrant_url, qdrant_api_key)\n",
        "    \n",
        "    # Initialize the embedding model\n",
        "    embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "    \n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\" Processing: {pdf_file if isinstance(pdf_file, str) else pdf_file.name}\")\n",
        "        \n",
        "        # Extract text from PDF\n",
        "        reader = PdfReader(pdf_file)\n",
        "        full_text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "        print(f\"    Extracted {len(full_text)} characters\")\n",
        "        \n",
        "        # Split into chunks\n",
        "        chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
        "        print(f\"    Created {len(chunks)} chunks\")\n",
        "        \n",
        "        # Create embeddings for all chunks\n",
        "        embeddings = embedding_model.get_embeddings(chunks)\n",
        "        \n",
        "        # Create points for Qdrant\n",
        "        points = []\n",
        "        for emb, chunk in zip(embeddings, chunks):\n",
        "            points.append(\n",
        "                models.PointStruct(\n",
        "                    id=str(uuid.uuid4()),  # Unique ID for each chunk\n",
        "                    vector=emb.values,      # The embedding vector\n",
        "                    payload={\"text\": chunk} # Store the original text\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        # Upsert points to Qdrant\n",
        "        client.upsert(collection_name=collection_name, points=points)\n",
        "        print(f\"    Uploaded {len(points)} chunks to Qdrant\")\n",
        "\n",
        "print(\" PDF ingestion function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Response Generation Function\n",
        "\n",
        "This is the **Retrieval + Generation** phase. The function:\n",
        "1. Takes the user's question\n",
        "2. Searches Qdrant for relevant document chunks\n",
        "3. Combines the context with the question\n",
        "4. Sends to Gemini to generate an answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Response generation function defined\n"
          ]
        }
      ],
      "source": [
        "def get_bot_response(messages: list, model_name: str, temperature: float, \n",
        "                     top_p: float, max_output: int, collection_name: str,\n",
        "                     qdrant_url: str, qdrant_api_key: str, k: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response using RAG: retrieve relevant context, then generate answer.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of conversation messages [{\"role\": \"user/assistant\", \"content\": \"...\"}]\n",
        "        model_name: Gemini model to use\n",
        "        temperature: Response randomness (0-1)\n",
        "        top_p: Response diversity (0-1)\n",
        "        max_output: Maximum output tokens\n",
        "        collection_name: Qdrant collection with document embeddings (None to skip RAG)\n",
        "        qdrant_url: Qdrant cluster URL\n",
        "        qdrant_api_key: Qdrant API key\n",
        "        k: Number of chunks to retrieve (default: 3)\n",
        "    \n",
        "    Returns:\n",
        "        Generated response text\n",
        "    \"\"\"\n",
        "    # Build conversation history for multi-turn conversations\n",
        "    history = \"\"\n",
        "    for msg in messages:\n",
        "        speaker = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "        history += f\"{speaker}: {msg['content']}\\n\"\n",
        "    \n",
        "    # RETRIEVAL: Get relevant context from Qdrant\n",
        "    context = \"\"\n",
        "    if collection_name and qdrant_url and qdrant_api_key:\n",
        "        # Create embedding for the user's question\n",
        "        embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "        query_embedding = embedding_model.get_embeddings([messages[-1][\"content\"]])[0].values\n",
        "        \n",
        "        # Search Qdrant for similar chunks\n",
        "        client = init_qdrant(qdrant_url, qdrant_api_key)\n",
        "        search_result = client.query_points(\n",
        "            collection_name=collection_name,\n",
        "            query=query_embedding,\n",
        "            limit=k\n",
        "        )\n",
        "        hits = search_result.points\n",
        "        \n",
        "        # Combine retrieved chunks into context\n",
        "        context = \"\\n\\n\".join(hit.payload.get(\"text\", \"\") for hit in hits)\n",
        "        print(f\" Retrieved {len(hits)} relevant chunks from documents\")\n",
        "    \n",
        "    # GENERATION: Create the prompt and generate response\n",
        "    if context:\n",
        "        prompt = f\"\"\"You are a helpful assistant. Use the following context from documents to answer the user's question. If the context doesn't contain relevant information, say so.\n",
        "\n",
        "Context from documents:\n",
        "{context}\n",
        "\n",
        "Conversation:\n",
        "{history}\n",
        "Assistant:\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"You are a helpful assistant.\n",
        "\n",
        "Conversation:\n",
        "{history}\n",
        "Assistant:\"\"\"\n",
        "    \n",
        "    # Call Gemini\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=model_name,\n",
        "        generation_config={\n",
        "            \"temperature\": temperature,\n",
        "            \"max_output_tokens\": max_output,\n",
        "            \"top_p\": top_p\n",
        "        }\n",
        "    )\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "print(\" Response generation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "##  Step 5: Test the RAG Pipeline (Notebook Version)\n",
        "\n",
        "Before running the full Streamlit app, let's test each component in the notebook.\n",
        "\n",
        "### 5.1 Test: Ingest a PDF\n",
        "\n",
        "First, let's ingest a sample PDF. You can use any PDF file you have available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Uncomment the code above and update PDF_PATH to test ingestion\n"
          ]
        }
      ],
      "source": [
        "# Test PDF Ingestion\n",
        "# Replace with the path to your PDF file\n",
        "\n",
        "PDF_PATH = \"data/your_document.pdf\"  # Update this path!\n",
        "\n",
        "# Check if credentials are set\n",
        "if not QDRANT_API_KEY or not QDRANT_URL:\n",
        "    print(\" Please set your Qdrant credentials in Step 3 first!\")\n",
        "else:\n",
        "    # Uncomment the lines below when you have a PDF to test\n",
        "    # print(\" Creating collection...\")\n",
        "    # create_qdrant_collection(\n",
        "    #     collection_name=DEFAULT_COLLECTION,\n",
        "    #     qdrant_url=QDRANT_URL,\n",
        "    #     qdrant_api_key=QDRANT_API_KEY,\n",
        "    #     vector_size=768  # gemini-embedding-001 dimension\n",
        "    # )\n",
        "    # \n",
        "    # print(\"\\nðŸ“¤ Ingesting PDF...\")\n",
        "    # ingest_pdfs_to_qdrant(\n",
        "    #     pdf_files=[PDF_PATH],\n",
        "    #     collection_name=DEFAULT_COLLECTION,\n",
        "    #     qdrant_url=QDRANT_URL,\n",
        "    #     qdrant_api_key=QDRANT_API_KEY,\n",
        "    #     chunk_size=500\n",
        "    # )\n",
        "    # print(\"\\n PDF ingestion complete!\")\n",
        "    \n",
        "    print(\" Uncomment the code above and update PDF_PATH to test ingestion\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Test: Ask a Question\n",
        "\n",
        "After ingesting a PDF, test the RAG pipeline by asking a question about the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Question: What is this document about?\n",
            "--------------------------------------------------\n",
            " Uncomment the code above after ingesting a PDF to test Q&A\n"
          ]
        }
      ],
      "source": [
        "# Test asking a question (after ingesting a PDF)\n",
        "\n",
        "# Your test question\n",
        "TEST_QUESTION = \"What is this document about?\"\n",
        "\n",
        "# Create a simple message history\n",
        "messages = [{\"role\": \"user\", \"content\": TEST_QUESTION}]\n",
        "\n",
        "# Check if we can run the test\n",
        "if not QDRANT_API_KEY or not QDRANT_URL:\n",
        "    print(\" Please set your Qdrant credentials first!\")\n",
        "else:\n",
        "    print(f\" Question: {TEST_QUESTION}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Uncomment below after ingesting a PDF\n",
        "    # response = get_bot_response(\n",
        "    #     messages=messages,\n",
        "    #     model_name=MODEL_NAME,\n",
        "    #     temperature=TEMPERATURE,\n",
        "    #     top_p=TOP_P,\n",
        "    #     max_output=MAX_OUTPUT,\n",
        "    #     collection_name=DEFAULT_COLLECTION,\n",
        "    #     qdrant_url=QDRANT_URL,\n",
        "    #     qdrant_api_key=QDRANT_API_KEY,\n",
        "    #     k=3  # Retrieve top 3 chunks\n",
        "    # )\n",
        "    # print(f\"\\n Answer:\\n{response}\")\n",
        "    \n",
        "    print(\" Uncomment the code above after ingesting a PDF to test Q&A\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Step 6: Launch the Streamlit Chatbot UI\n",
        "\n",
        "The code below creates a beautiful web UI for your RAG chatbot using service called Streamlit.\n",
        "\n",
        "Dont worry about the Streamlit code for now, its just an meant to be an easy way to launch our UI and work with the techniques we've learned above.\n",
        "\n",
        "### How to Run (3 Easy Steps):\n",
        "\n",
        "1. **Step 6.part1:** Run the first cell below to create the `rag_chatbot_app.py` file\n",
        "2. **Step 6.part2:** Run the second cell to launch the Streamlit app\n",
        "3. **Open the URL** that appears (usually `http://localhost:8501`)\n",
        "\n",
        "### The UI Features:\n",
        "-  PDF upload functionality  \n",
        "-  Interactive chat interface\n",
        "-  Multi-turn conversation support\n",
        "-  Real-time RAG responses\n",
        "\n",
        "> ** Important**: Make sure you've filled in your `QDRANT_API_KEY` and `QDRANT_URL` in Step 3 before running!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Creating rag_chatbot_app.py...\n",
            "   Using QDRANT_URL: https://d78b1147-cde0-4b94-aa1e-9b6b2278...\n",
            "   Using MODEL: gemini-2.0-flash-001\n",
            "\n",
            " File created: rag_chatbot_app.py\n",
            "\n",
            " Now run the next cell to launch the app!\n"
          ]
        }
      ],
      "source": [
        "#  Step 6-part1: Create the Streamlit app file\n",
        "# This cell writes a complete Python file with all dependencies\n",
        "\n",
        "# First, let's save the credentials to include in the file\n",
        "print(\" Creating rag_chatbot_app.py...\")\n",
        "print(f\"   Using QDRANT_URL: {QDRANT_URL[:40]}...\" if QDRANT_URL else \"   âš ï¸ QDRANT_URL is empty!\")\n",
        "print(f\"   Using MODEL: {MODEL_NAME}\")\n",
        "\n",
        "# Write the file with the current configuration\n",
        "app_code = f'''# RAG PDF Chatbot - Streamlit Application\n",
        "# Generated from Lab 3 Notebook\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "import vertexai\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION (from notebook)\n",
        "# ============================================\n",
        "QDRANT_API_KEY = \"{QDRANT_API_KEY}\"\n",
        "QDRANT_URL = \"{QDRANT_URL}\"\n",
        "MODEL_NAME = \"{MODEL_NAME}\"\n",
        "TEMPERATURE = {TEMPERATURE}\n",
        "TOP_P = {TOP_P}\n",
        "MAX_OUTPUT = {MAX_OUTPUT}\n",
        "DEFAULT_COLLECTION = \"{DEFAULT_COLLECTION}\"\n",
        "\n",
        "# Initialize Google AI\n",
        "GOOGLE_API_KEY = \"{GOOGLE_API_KEY if 'GOOGLE_API_KEY' in dir() else ''}\"\n",
        "if GOOGLE_API_KEY:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "else:\n",
        "    genai.configure()\n",
        "\n",
        "# Initialize Vertex AI\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"{PROJECT_ID if 'PROJECT_ID' in dir() else 'your-project-id'}\")\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
        "\n",
        "# ============================================\n",
        "# RAG FUNCTIONS\n",
        "# ============================================\n",
        "def init_qdrant(qdrant_url, qdrant_api_key):\n",
        "    return QdrantClient(url=qdrant_url, api_key=qdrant_api_key, timeout=30)\n",
        "\n",
        "def create_qdrant_collection(collection_name, qdrant_url, qdrant_api_key, vector_size=768, distance=\"Cosine\"):\n",
        "    client = init_qdrant(qdrant_url, qdrant_api_key)\n",
        "    try:\n",
        "        client.recreate_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(size=vector_size, distance=distance)\n",
        "        )\n",
        "    except:\n",
        "        try:\n",
        "            client.delete_collection(collection_name=collection_name)\n",
        "        except:\n",
        "            pass\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(size=vector_size, distance=distance)\n",
        "        )\n",
        "\n",
        "def ingest_pdfs_to_qdrant(pdf_files, collection_name, qdrant_url, qdrant_api_key, chunk_size=500):\n",
        "    client = init_qdrant(qdrant_url, qdrant_api_key)\n",
        "    embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "    \n",
        "    for pdf_file in pdf_files:\n",
        "        reader = PdfReader(pdf_file)\n",
        "        full_text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "        chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
        "        embeddings = embedding_model.get_embeddings(chunks)\n",
        "        \n",
        "        points = []\n",
        "        for emb, chunk in zip(embeddings, chunks):\n",
        "            points.append(models.PointStruct(\n",
        "                id=str(uuid.uuid4()),\n",
        "                vector=emb.values,\n",
        "                payload={{\"text\": chunk}}\n",
        "            ))\n",
        "        client.upsert(collection_name=collection_name, points=points)\n",
        "\n",
        "def get_bot_response(messages, model_name, temperature, top_p, max_output,\n",
        "                     collection_name, qdrant_url, qdrant_api_key, k=3):\n",
        "    history = \"\"\n",
        "    for msg in messages:\n",
        "        speaker = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "        history += f\"{{speaker}}: {{msg['content']}}\\\\n\"\n",
        "    \n",
        "    context = \"\"\n",
        "    if collection_name and qdrant_url and qdrant_api_key:\n",
        "        embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "        query_embedding = embedding_model.get_embeddings([messages[-1][\"content\"]])[0].values\n",
        "        client = init_qdrant(qdrant_url, qdrant_api_key)\n",
        "        search_result = client.query_points(collection_name=collection_name, query=query_embedding, limit=k)\n",
        "        hits = search_result.points\n",
        "        context = \"\\\\n\\\\n\".join(hit.payload.get(\"text\", \"\") for hit in hits)\n",
        "    \n",
        "    if context:\n",
        "        prompt = f\"\"\"You are a helpful assistant. Use the following context to answer questions.\n",
        "\n",
        "Context from documents:\n",
        "{{context}}\n",
        "\n",
        "Conversation:\n",
        "{{history}}\n",
        "Assistant:\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"You are a helpful assistant.\n",
        "\n",
        "Conversation:\n",
        "{{history}}\n",
        "Assistant:\"\"\"\n",
        "    \n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=model_name,\n",
        "        generation_config={{\"temperature\": temperature, \"max_output_tokens\": max_output, \"top_p\": top_p}}\n",
        "    )\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "# ============================================\n",
        "# STREAMLIT UI\n",
        "# ============================================\n",
        "st.set_page_config(page_title=\"My PDF RAG Chatbot\", layout=\"wide\", page_icon=\"\")\n",
        "st.title(\" Chat with My PDFs\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"qdrant_collection\" not in st.session_state:\n",
        "    st.session_state.qdrant_collection = None\n",
        "\n",
        "uploaded_files = st.file_uploader(\n",
        "    \" Upload PDFs (optional â€” chat works even without PDFs)\",\n",
        "    type=[\"pdf\"],\n",
        "    accept_multiple_files=True,\n",
        ")\n",
        "\n",
        "if uploaded_files and not st.session_state.qdrant_collection:\n",
        "    with st.spinner(\"Creating Qdrant collection & ingesting PDFsâ€¦\"):\n",
        "        create_qdrant_collection(\n",
        "            collection_name=DEFAULT_COLLECTION,\n",
        "            qdrant_url=QDRANT_URL,\n",
        "            qdrant_api_key=QDRANT_API_KEY,\n",
        "        )\n",
        "        ingest_pdfs_to_qdrant(\n",
        "            pdf_files=uploaded_files,\n",
        "            collection_name=DEFAULT_COLLECTION,\n",
        "            qdrant_url=QDRANT_URL,\n",
        "            qdrant_api_key=QDRANT_API_KEY,\n",
        "        )\n",
        "        st.session_state.qdrant_collection = DEFAULT_COLLECTION\n",
        "        st.success(\" PDFs ingested! Future replies will include RAG context.\")\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.markdown(msg[\"content\"])\n",
        "\n",
        "if user_input := st.chat_input(\"Type your questionâ€¦\"):\n",
        "    st.session_state.messages.append({{\"role\": \"user\", \"content\": user_input}})\n",
        "    st.chat_message(\"user\").markdown(user_input)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinkingâ€¦\"):\n",
        "            reply = get_bot_response(\n",
        "                messages=st.session_state.messages,\n",
        "                model_name=MODEL_NAME,\n",
        "                temperature=TEMPERATURE,\n",
        "                top_p=TOP_P,\n",
        "                max_output=MAX_OUTPUT,\n",
        "                collection_name=st.session_state.qdrant_collection,\n",
        "                qdrant_url=QDRANT_URL,\n",
        "                qdrant_api_key=QDRANT_API_KEY,\n",
        "            )\n",
        "            st.markdown(reply)\n",
        "\n",
        "    st.session_state.messages.append({{\"role\": \"assistant\", \"content\": reply}})\n",
        "'''\n",
        "\n",
        "# Write the file\n",
        "with open(\"rag_chatbot_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"\\n File created: rag_chatbot_app.py\")\n",
        "print(\"\\n Now run the next cell to launch the app!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Launching Streamlit app...\n",
            "   This will open a new browser tab automatically!\n",
            "\n",
            " To stop the app: Click 'Interrupt Kernel' or press Ctrl+C\n",
            "--------------------------------------------------\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Step 6-part2: Launch the Streamlit App\n",
        "# Run this cell to start the chatbot server\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\" Launching Streamlit app...\")\n",
        "print(\"   This will open a new browser tab automatically!\")\n",
        "print(\"\\n To stop the app: Click 'Interrupt Kernel' or press Ctrl+C\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Run streamlit using Python module syntax (more reliable)\n",
        "!{sys.executable} -m streamlit run rag_chatbot_app.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Working RAG Chatbot\n",
        "\n",
        "Here's what the chatbot looks like in action! This example shows a user uploading the famous \"Attention Is All You Need\" paper and asking questions about it:\n",
        "\n",
        "![RAG Chatbot Demo](images/streamlit_demo.png)\n",
        "\n",
        "**What's happening:**\n",
        "1.  We uploaded \"Attention Is All You Need.pdf\"\n",
        "2.  We asked: \"What is this document about?\"\n",
        "3.  The chatbot retrieved relevant chunks from the PDF and generated an accurate response about the Transformer architecture paper!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª Lab Exercises\n",
        "\n",
        "Now it's your turn! Complete these exercises to deepen your understanding.\n",
        "\n",
        "### Lab Exercise 1: Experiment with Chunk Size\n",
        "\n",
        "The `chunk_size` parameter affects how text is split. Smaller chunks = more precise retrieval but less context. Larger chunks = more context but might include irrelevant information.\n",
        "\n",
        "**Task**: Modify the ingestion function to use different chunk sizes and observe how it affects retrieval quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Lab Exercise 1: Experiment with Chunk Sizes\n",
        "# Try ingesting the same PDF with different chunk sizes\n",
        "\n",
        "CHUNK_SIZES_TO_TEST = [250, 500, 1000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lab Exercise 2: Adjust Top-K Retrieved Chunks\n",
        "\n",
        "The `k` parameter in `get_bot_response` controls how many chunks are retrieved. More chunks = more context but potentially more noise.\n",
        "\n",
        "**Task**: Test the same question with k=1, k=3, and k=5. How does the response quality change?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Lab Exercise 2: Test Different k Values\n",
        "# How many chunks should we retrieve?\n",
        "\n",
        "K_VALUES_TO_TEST = [1, 3, 5]\n",
        "TEST_QUESTION_EX2 = \"What are the main topics covered in this document?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lab Exercise 3: Customize the System Prompt\n",
        "\n",
        "The current prompt in `get_bot_response` is generic. Customize it for a specific use case!\n",
        "\n",
        "**Task**: Modify the prompt to make the chatbot act as a specific persona (e.g., a technical support agent, a research assistant, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab Exercise 3: Custom System Prompt\n",
        "# Create a specialized chatbot persona\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ‰ Conclusion\n",
        "\n",
        "Congratulations! You've built a complete **RAG (Retrieval Augmented Generation)** application!\n",
        "\n",
        "### What You've Learned:\n",
        "\n",
        "| Concept | What It Does |\n",
        "|---------|--------------|\n",
        "| **RAG Pipeline** | Combines retrieval + generation for accurate answers |\n",
        "| **PDF Processing** | Extract and chunk text from documents |\n",
        "| **Embeddings** | Convert text to vectors for similarity search |\n",
        "| **Vector Search** | Find relevant document chunks |\n",
        "| **Prompt Engineering** | Combine context + question for LLM |\n",
        "| **Streamlit** | Build interactive web applications |\n",
        "\n",
        "### Key Parameters to Tune:\n",
        "\n",
        "| Parameter | Effect | Typical Range |\n",
        "|-----------|--------|---------------|\n",
        "| `chunk_size` | How much context per chunk | 250-1000 characters |\n",
        "| `k` (top-k) | How many chunks to retrieve | 1-5 chunks |\n",
        "| `temperature` | Response creativity | 0.0-1.0 |\n",
        "| `vector_size` | Embedding dimensions | Model-dependent |\n",
        "\n",
        "### Next Steps:\n",
        "1.  Try with your own PDF documents\n",
        "2.  Customize the Streamlit UI\n",
        "3.  Experiment with different embedding models\n",
        "4.  Add metadata filtering (like in Lab 2)\n",
        "5.  Deploy your chatbot to the cloud!\n",
        "\n",
        "Note to see how your Lab Answers compare, go to the Solutions Folder to see our answers!\n",
        "\n",
        "### Additional Resources:\n",
        "- [Qdrant Documentation](https://qdrant.tech/documentation/)\n",
        "- [Google Gemini API](https://ai.google.dev/docs)\n",
        "- [Streamlit Documentation](https://docs.streamlit.io/)\n",
        "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
        "\n",
        "---\n",
        "\n",
        "** You've completed all 3 labs! You now have hands-on experience with:**\n",
        "-  **Lab 1**: LLM prompting and structured output\n",
        "-  **Lab 2**: Vector databases and similarity search\n",
        "-  **Lab 3**: Full RAG application with UI\n",
        "\n",
        "Happy building!!! \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
